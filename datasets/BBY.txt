Embedding Wikipedia articles for search
Embedding Wikipedia articles for search
This notebook shows how we prepared a dataset of Wikipedia articles for search, used in
Question_answering_using_embeddings.ipynb
.
Procedure:
1
. 
Prerequisites: Import libraries, set API key (if needed)
2
. 
Collect: We download a few hundred Wikipedia articles about the 2022 Olympics
3
. 
Chunk: Documents are split into short, semi-self-contained sections to be embedded
4
. 
Embed: Each section is embedded with the OpenAI API
5
. 
Store: Embeddings are saved in a CSV file (for large datasets, use a vector database)
0. Prerequisites
0. Prerequisites
Import libraries
Import libraries
In [1]:
# imports
import
import
 
mwclient
mwclient
  
# for downloading example Wikipedia articles
import
import
 
mwparserfromhell
mwparserfromhell
  
# for splitting Wikipedia articles into sections
import
import
 
openai
openai
  
# for generating embeddings
import
import
 
pandas
pandas
 
as
as
 
pd
pd
  
# for DataFrames to store article sections and embeddings
import
import
 
re
re
  
# for cutting <ref> links out of Wikipedia articles
import
import
 
tiktoken
tiktoken
  
# for counting tokens
Install any missing libraries with 
pip install
 in your terminal. E.g.,
pip install openai
(You can also do this in a notebook cell with 
!pip install openai
.)
If you install any libraries, be sure to restart the notebook kernel.
Set API key (if needed)
Set API key (if needed)
Note that the OpenAI library will try to read your API key from the 
OPENAI_API_KEY
 environment variable. If
you haven't already, set this environment variable by following 
these instructions
.
1. Collect documents
1. Collect documents
In this example, we'll download a few hundred Wikipedia articles related to the 2022 Winter Olympics.
In [2]:
# get Wikipedia pages about the 2022 Winter Olympics
CATEGORY_TITLE
 
=
 
"Category:2022 Winter Olympics"
WIKI_SITE
 
=
 
"en.wikipedia.org"
def
def
 
titles_from_category
(
    
category
:
 
mwclient
.
listing
.
Category
,
 
max_depth
:
 
int
)
 
->
 
set
[
str
]:
    
"""Return a set of page titles in a given Wiki category and its subcategories."""
    
titles
 
=
 
set
()
    
for
for
 
cm
 
in
in
 
category
.
members
():        
if
if
 
type
(
cm
)
 
==
 
mwclient
.
page
.
Page
:
            
# ^type() used instead of isinstance() to catch match w/ no inheritance
            
titles
.
add
(
cm
.
name
)
        
elif
elif
 
isinstance
(
cm
,
 
mwclient
.
listing
.
Category
)
 
and
and
 
max_depth
 
>
 
0
:
            
deeper_titles
 
=
 
titles_from_category
(
cm
,
 
max_depth
=
max_depth
 
-
 
1
)
            
titles
.
update
(
deeper_titles
)
    
return
return
 
titles
site
 
=
 
mwclient
.
Site
(
WIKI_SITE
)
category_page
 
=
 
site
.
pages
[
CATEGORY_TITLE
]
titles
 
=
 
titles_from_category
(
category_page
,
 
max_depth
=
1
)
# ^note: max_depth=1 means we go one level deep in the category tree
print
(
f
"Found {len(titles)} article titles in 
{CATEGORY_TITLE}
{CATEGORY_TITLE}
."
)
2. Chunk documents
2. Chunk documents
Now that we have our reference documents, we need to prepare them for search.
Because GPT can only read a limited amount of text at once, we'll split each document into chunks short
enough to be read.
For this specific example on Wikipedia articles, we'll:
Discard less relevant-looking sections like External Links and Footnotes
Clean up the text by removing reference tags (e.g., 
), whitespace, and super short sections
Split each article into sections
Prepend titles and subtitles to each section's text, to help GPT understand the context
If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to split along
semantic boundaries like paragraphs
In [3]:
# define functions to split Wikipedia pages into sections
SECTIONS_TO_IGNORE
 
=
 
[
    
"See also"
,
    
"References"
,
    
"External links"
,
    
"Further reading"
,
    
"Footnotes"
,
    
"Bibliography"
,
    
"Sources"
,
    
"Citations"
,
    
"Literature"
,
    
"Footnotes"
,
    
"Notes and references"
,
    
"Photo gallery"
,
    
"Works cited"
,
    
"Photos"
,
    
"Gallery"
,
    
"Notes"
,
    
"References and sources"
,
    
"References and notes"
,
]
def
def
 
all_subsections_from_section
(
    
section
:
 
mwparserfromhell
.
wikicode
.
Wikicode
,
    
parent_titles
:
 
list
[
str
],
    
sections_to_ignore
:
 
set
[
str
],
)
 
->
 
list
[
tuple
[
list
[
str
],
 
str
]]:
    
"""
    From a Wikipedia section, return a flattened list of all nested subsections.
    Each subsection is a tuple, where:
        - the first element is a list of parent subtitles, starting with the page title
Found 731 article titles in Category:2022 Winter Olympics.        - the second element is the text of the subsection (but not any children)
    """
    
headings
 
=
 
[
str
(
h
)
 
for
for
 
h
 
in
in
 
section
.
filter_headings
()]
    
title
 
=
 
headings
[
0
]
    
if
if
 
title
.
strip
(
"="
 
+
 
" "
)
 
in
in
 
sections_to_ignore
:
        
# ^wiki headings are wrapped like "== Heading =="
        
return
return
 
[]
    
titles
 
=
 
parent_titles
 
+
 
[
title
]
    
full_text
 
=
 
str
(
section
)
    
section_text
 
=
 
full_text
.
split
(
title
)[
1
]
    
if
if
 
len
(
headings
)
 
==
 
1
:
        
return
return
 
[(
titles
,
 
section_text
)]
    
else
else
:
        
first_subtitle
 
=
 
headings
[
1
]
        
section_text
 
=
 
section_text
.
split
(
first_subtitle
)[
0
]
        
results
 
=
 
[(
titles
,
 
section_text
)]
        
for
for
 
subsection
 
in
in
 
section
.
get_sections
(
levels
=
[
len
(
titles
)
 
+
 
1
]):
            
results
.
extend
(
all_subsections_from_section
(
subsection
,
 
titles
,
 
sections_to_
ignore
))
        
return
return
 
results
def
def
 
all_subsections_from_title
(
    
title
:
 
str
,
    
sections_to_ignore
:
 
set
[
str
]
 
=
 
SECTIONS_TO_IGNORE
,
    
site_name
:
 
str
 
=
 
WIKI_SITE
,
)
 
->
 
list
[
tuple
[
list
[
str
],
 
str
]]:
    
"""From a Wikipedia page title, return a flattened list of all nested subsections.
    Each subsection is a tuple, where:
        - the first element is a list of parent subtitles, starting with the page title
        - the second element is the text of the subsection (but not any children)
    """
    
site
 
=
 
mwclient
.
Site
(
site_name
)
    
page
 
=
 
site
.
pages
[
title
]
    
text
 
=
 
page
.
text
()
    
parsed_text
 
=
 
mwparserfromhell
.
parse
(
text
)
    
headings
 
=
 
[
str
(
h
)
 
for
for
 
h
 
in
in
 
parsed_text
.
filter_headings
()]
    
if
if
 
headings
:
        
summary_text
 
=
 
str
(
parsed_text
)
.
split
(
headings
[
0
])[
0
]
    
else
else
:
        
summary_text
 
=
 
str
(
parsed_text
)
    
results
 
=
 
[([
title
],
 
summary_text
)]
    
for
for
 
subsection
 
in
in
 
parsed_text
.
get_sections
(
levels
=
[
2
]):
        
results
.
extend
(
all_subsections_from_section
(
subsection
,
 
[
title
],
 
sections_to_ign
ore
))
    
return
return
 
results
In [4]:
# split pages into sections
# may take ~1 minute per 100 articles
wikipedia_sections
 
=
 
[]
for
for
 
title
 
in
in
 
titles
:
    
wikipedia_sections
.
extend
(
all_subsections_from_title
(
title
))
print
(
f
"Found {len(wikipedia_sections)} sections in {len(titles)} pages."
)
In [5]:
# clean text
def
def
 
clean_section
(
section
:
 
tuple
[
list
[
str
],
 
str
])
 
->
 
tuple
[
list
[
str
],
 
str
]:
    
"""
    Return a cleaned up section with:
        - <ref>xyz</ref> patterns removed
        - leading/trailing whitespace removed
    """
    
titles
,
 
text
 
=
 
section
    
text
 
=
 
re
.
sub
(
r
"<ref.*?</ref>"
,
 
""
,
 
text
)
    
text
 
=
 
text
.
strip
()
    
return
return
 
(
titles
,
 
text
)
Found 5730 sections in 731 pages.wikipedia_sections
 
=
 
[
clean_section
(
ws
)
 
for
for
 
ws
 
in
in
 
wikipedia_sections
]
# filter out short/blank sections
def
def
 
keep_section
(
section
:
 
tuple
[
list
[
str
],
 
str
])
 
->
 
bool
:
    
"""Return True if the section should be kept, False otherwise."""
    
titles
,
 
text
 
=
 
section
    
if
if
 
len
(
text
)
 
<
 
16
:
        
return
return
 
False
False
    
else
else
:
        
return
return
 
True
True
original_num_sections
 
=
 
len
(
wikipedia_sections
)
wikipedia_sections
 
=
 
[
ws
 
for
for
 
ws
 
in
in
 
wikipedia_sections
 
if
if
 
keep_section
(
ws
)]
print
(
f
"Filtered out {original_num_sections-len(wikipedia_sections)} sections, leaving {l
en(wikipedia_sections)} sections."
)
In [6]:
# print example data
for
for
 
ws
 
in
in
 
wikipedia_sections
[:
5
]:
    
print
(
ws
[
0
])
    
display
(
ws
[
1
][:
77
]
 
+
 
"..."
)
    
print
()
Next, we'll recursively split long sections into smaller sections.
There's no perfect recipe for splitting text into sections.
Some tradeoffs include:
Longer sections may be better for questions that require more context
Longer sections may be worse for retrieval, as they may have more topics muddled together
Shorter sections are better for reducing costs (which are proportional to the number of tokens)
Shorter sections allow more sections to be retrieved, which may help with recall
Overlapping sections may help prevent answers from being cut by section boundaries
Here, we'll use a simple approach and limit sections to 1,600 tokens each, recursively halving any sections that
are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when
possible.
In [7]:
GPT_MODEL
 
=
 
"gpt-3.5-turbo"
  
# only matters insofar as it selects which tokenizer to use
Filtered out 530 sections, leaving 5200 sections.
['Lviv bid for the 2022 Winter Olympics']
'{{Olympic bid|2022|Winter|\n| Paralympics = yes\n| logo = Lviv 2022 Winter Olym...'
['Lviv bid for the 2022 Winter Olympics', '==History==']
'[[Image:LwÃ³w - Rynek 01.JPG|thumb|right|200px|View of Rynok Square in Lviv]]\n...'
['Lviv bid for the 2022 Winter Olympics', '==Venues==']
'{{Location map+\n|Ukraine\n|border =\n|caption = Venue areas\n|float = left\n|widt...'
['Lviv bid for the 2022 Winter Olympics', '==Venues==', '===City zone===']
'The main Olympic Park would be centered around the [[Arena Lviv]], hosting th...'
['Lviv bid for the 2022 Winter Olympics', '==Venues==', '===Mountain zone===', '====Venue
cluster Tysovets-Panasivka====']
'An existing military ski training facility in [[Tysovets, Skole Raion|Tysovet...'def
def
 
num_tokens
(
text
:
 
str
,
 
model
:
 
str
 
=
 
GPT_MODEL
)
 
->
 
int
:
    
"""Return the number of tokens in a string."""
    
encoding
 
=
 
tiktoken
.
encoding_for_model
(
model
)
    
return
return
 
len
(
encoding
.
encode
(
text
))
def
def
 
halved_by_delimiter
(
string
:
 
str
,
 
delimiter
:
 
str
 
=
 
"
\n
\n
"
)
 
->
 
list
[
str
,
 
str
]:
    
"""Split a string in two, on a delimiter, trying to balance tokens on each side."""
    
chunks
 
=
 
string
.
split
(
delimiter
)
    
if
if
 
len
(
chunks
)
 
==
 
1
:
        
return
return
 
[
string
,
 
""
]
  
# no delimiter found
    
elif
elif
 
len
(
chunks
)
 
==
 
2
:
        
return
return
 
chunks
  
# no need to search for halfway point
    
else
else
:
        
total_tokens
 
=
 
num_tokens
(
string
)
        
halfway
 
=
 
total_tokens
 
//
 
2
        
best_diff
 
=
 
halfway
        
for
for
 
i
,
 
chunk
 
in
in
 
enumerate
(
chunks
):
            
left
 
=
 
delimiter
.
join
(
chunks
[:
 
i
 
+
 
1
])
            
left_tokens
 
=
 
num_tokens
(
left
)
            
diff
 
=
 
abs
(
halfway
 
-
 
left_tokens
)
            
if
if
 
diff
 
>=
 
best_diff
:
                
break
break
            
else
else
:
                
best_diff
 
=
 
diff
        
left
 
=
 
delimiter
.
join
(
chunks
[:
i
])
        
right
 
=
 
delimiter
.
join
(
chunks
[
i
:])
        
return
return
 
[
left
,
 
right
]
def
def
 
truncated_string
(
    
string
:
 
str
,
    
model
:
 
str
,
    
max_tokens
:
 
int
,
    
print_warning
:
 
bool
 
=
 
True
True
,
)
 
->
 
str
:
    
"""Truncate a string to a maximum number of tokens."""
    
encoding
 
=
 
tiktoken
.
encoding_for_model
(
model
)
    
encoded_string
 
=
 
encoding
.
encode
(
string
)
    
truncated_string
 
=
 
encoding
.
decode
(
encoded_string
[:
max_tokens
])
    
if
if
 
print_warning
 
and
and
 
len
(
encoded_string
)
 
>
 
max_tokens
:
        
print
(
f
"Warning: Truncated string from {len(encoded_string)} tokens to 
{max_toke
{max_toke
ns}
ns}
 tokens."
)
    
return
return
 
truncated_string
def
def
 
split_strings_from_subsection
(
    
subsection
:
 
tuple
[
list
[
str
],
 
str
],
    
max_tokens
:
 
int
 
=
 
1000
,
    
model
:
 
str
 
=
 
GPT_MODEL
,
    
max_recursion
:
 
int
 
=
 
5
,
)
 
->
 
list
[
str
]:
    
"""
    Split a subsection into a list of subsections, each with no more than max_tokens.
    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).
    """
    
titles
,
 
text
 
=
 
subsection
    
string
 
=
 
"
\n\n
\n\n
"
.
join
(
titles
 
+
 
[
text
])
    
num_tokens_in_string
 
=
 
num_tokens
(
string
)
    
# if length is fine, return string
    
if
if
 
num_tokens_in_string
 
<=
 
max_tokens
:
        
return
return
 
[
string
]
    
# if recursion hasn't found a split after X iterations, just truncate
    
elif
elif
 
max_recursion
 
==
 
0
:
        
return
return
 
[
truncated_string
(
string
,
 
model
=
model
,
 
max_tokens
=
max_tokens
)]
    
# otherwise, split in half and recurse
    
else
else
:
        
titles
,
 
text
 
=
 
subsection
        
for
for
 
delimiter
 
in
in
 
[
"
\n\n
\n\n
"
,
 
"
\n
\n
"
,
 
". "
]:
            
left
,
 
right
 
=
 
halved_by_delimiter
(
text
,
 
delimiter
=
delimiter
)
            
if
if
 
left
 
==
 
""
 
or
or
 
right
 
==
 
""
:
                
# if either half is empty, retry with a more fine-grained delimiter                
continue
continue
            
else
else
:
                
# recurse on each half
                
results
 
=
 
[]
                
for
for
 
half
 
in
in
 
[
left
,
 
right
]:
                    
half_subsection
 
=
 
(
titles
,
 
half
)
                    
half_strings
 
=
 
split_strings_from_subsection
(
                        
half_subsection
,
                        
max_tokens
=
max_tokens
,
                        
model
=
model
,
                        
max_recursion
=
max_recursion
 
-
 
1
,
                    
)
                    
results
.
extend
(
half_strings
)
                
return
return
 
results
    
# otherwise no split was found, so just truncate (should be very rare)
    
return
return
 
[
truncated_string
(
string
,
 
model
=
model
,
 
max_tokens
=
max_tokens
)]
In [8]:
# split sections into chunks
MAX_TOKENS
 
=
 
1600
wikipedia_strings
 
=
 
[]
for
for
 
section
 
in
in
 
wikipedia_sections
:
    
wikipedia_strings
.
extend
(
split_strings_from_subsection
(
section
,
 
max_tokens
=
MAX_TOKEN
S
))
print
(
f
"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)}
 
strings."
)
In [9]:
# print example data
print
(
wikipedia_strings
[
1
])
5200 Wikipedia sections split into 6059 strings.
Lviv bid for the 2022 Winter Olympics
==History==
[[Image:LwÃ³w - Rynek 01.JPG|thumb|right|200px|View of Rynok Square in Lviv]]
On 27 May 2010, [[President of Ukraine]] [[Viktor Yanukovych]] stated during a visit to [
[Lviv]] that Ukraine "will start working on the official nomination of our country as the
holder of the Winter Olympic Games in [[Carpathian Mountains|Carpathians]]".
In September 2012, [[government of Ukraine]] approved a document about the technical-econ
omic substantiation of the national project "Olympic Hope 2022". This was announced by Vl
adyslav Kaskiv, the head of UkraineÂ´s Derzhinvestproekt (State investment project). The o
rganizers announced on their website venue plans featuring Lviv as the host city and loca
tion for the "ice sport" venues, [[Volovets]] (around {{convert|185|km|mi|abbr=on}} from
 
Lviv) as venue for the [[Alpine skiing]] competitions and [[Tysovets, Skole Raion|Tysovet
s]] (around {{convert|130|km|mi|abbr=on}} from Lviv) as venue for all other "snow sport"
 
competitions. By March 2013 no other preparations than the feasibility study had been app
roved.
On 24 October 2013, session of the Lviv City Council adopted a resolution "About submissi
on to the International Olympic Committee for nomination of city to participate in the pr
ocedure for determining the host city of Olympic and Paralympic Winter Games in 2022".
On 5 November 2013, it was confirmed that Lviv was bidding to host the [[2022 Winter Olym
pics]]. Lviv would host the ice sport events, while the skiing events would be held in th
e [[Carpathian]] mountains. This was the first bid Ukraine had ever submitted for an Olym
pic Games.
On 30 June 2014, the International Olympic Committee announced "Lviv will turn its attent
ion to an Olympic bid for 2026, and not continue with its application for 2022. The decis
ion comes as a result of the present political and economic circumstances in Ukraine."
Ukraine's Deputy Prime Minister Oleksandr Vilkul said that the Winter Games "will be an i
mpetus not just for promotion of sports and tourism in Ukraine, but a very important comp
onent in the economic development of Ukraine, the attraction of the investments, the crea3. Embed document chunks
3. Embed document chunks
Now that we've split our library into shorter self-contained strings, we can compute embeddings for each.
(For large embedding jobs, use a script like 
api_request_parallel_processor.py
 to parallelize requests while
throttling to stay under rate limits.)
In [10]:
# calculate embeddings
EMBEDDING_MODEL
 
=
 
"text-embedding-ada-002"
  
# OpenAI's best embeddings as of Apr 2023
BATCH_SIZE
 
=
 
1000
  
# you can submit up to 2048 embedding inputs per request
embeddings
 
=
 
[]
for
for
 
batch_start
 
in
in
 
range
(
0
,
 
len
(
wikipedia_strings
),
 
BATCH_SIZE
):
    
batch_end
 
=
 
batch_start
 
+
 
BATCH_SIZE
    
batch
 
=
 
wikipedia_strings
[
batch_start
:
batch_end
]
    
print
(
f
"Batch 
{batch_start}
{batch_start}
 to {batch_end-1}"
)
    
response
 
=
 
openai
.
Embedding
.
create
(
model
=
EMBEDDING_MODEL
,
 
input
=
batch
)
    
for
for
 
i
,
 
be
 
in
in
 
enumerate
(
response
[
"data"
]):
        
assert
assert
 
i
 
==
 
be
[
"index"
]
  
# double check embeddings are in same order as input
    
batch_embeddings
 
=
 
[
e
[
"embedding"
]
 
for
for
 
e
 
in
in
 
response
[
"data"
]]
    
embeddings
.
extend
(
batch_embeddings
)
df
 
=
 
pd
.
DataFrame
({
"text"
:
 
wikipedia_strings
,
 
"embedding"
:
 
embeddings
})
4. Store document chunks and embeddings
4. Store document chunks and embeddings
Because this example only uses a few thousand strings, we'll store them in a CSV file.
(For larger datasets, use a vector database, which will be more performant.)
In [11]:
# save document chunks and embeddings
SAVE_PATH
 
=
 
"data/winter_olympics_2022.csv"
df
.
to_csv
(
SAVE_PATH
,
 
index
=
False
False
)
tion of new jobs, opening Ukraine to the world, returning Ukrainians working abroad to th
eir motherland."
Lviv was one of the host cities of [[UEFA Euro 2012]].
Batch 0 to 999
Batch 1000 to 1999
Batch 2000 to 2999
Batch 3000 to 3999
Batch 4000 to 4999
Batch 5000 to 5999
Batch 6000 to 6999